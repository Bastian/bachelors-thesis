% Encoding: UTF-8

@Misc{1706.03762,
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title  = {Attention Is All You Need},
  year   = {2017},
  eprint = {arXiv:1706.03762},
}

@InProceedings{Mccowan05theami,
  author    = {I. Mccowan and G. Lathoud and M. Lincoln and A. Lisowska and W. Post and D. Reidsma and P. Wellner},
  title     = {The AMI Meeting Corpus},
  booktitle = {In: Proceedings Measuring Behavior 2005, 5th International Conference on Methods and Techniques in Behavioral Research. L.P.J.J. Noldus, F. Grieco, L.W.S. Loijens and P.H. Zimmerman (Eds.), Wageningen: Noldus Information Technology},
  year      = {2005},
}

@InProceedings{Janin,
  author    = {A. Janin and D. Baron and J. Edwards and D. Ellis and D. Gelbart and N. Morgan and B. Peskin and T. Pfau and E. Shriberg and A. Stolcke and C. Wooters},
  title     = {The {ICSI} Meeting Corpus},
  booktitle = {2003 {IEEE} International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. ({ICASSP} {\textquotesingle}03).},
  year      = {2003},
  publisher = {{IEEE}},
  doi       = {10.1109/icassp.2003.1198793},
}

@Article{devlin2018bert,
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title   = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  journal = {arXiv preprint arXiv:1810.04805},
  year    = {2018},
}

@InProceedings{rajpurkar-etal-2016-squad,
  author    = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  title     = {{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  year      = {2016},
  pages     = {2383--2392},
  address   = {Austin, Texas},
  month     = nov,
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/D16-1264},
  url       = {https://www.aclweb.org/anthology/D16-1264},
}

@Misc{1804.07461,
  author = {Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
  title  = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  year   = {2018},
  eprint = {arXiv:1804.07461},
}

@Misc{1908.08345,
  author = {Yang Liu and Mirella Lapata},
  title  = {Text Summarization with Pretrained Encoders},
  year   = {2019},
  eprint = {arXiv:1908.08345},
}

@Article{1902.09243,
  author       = {Haoyu Zhang and Jianjun Xu and Ji Wang},
  title        = {Pretraining-Based Natural Language Generation for Text Summarization},
  year         = {2019},
  eprint       = {arXiv:1902.09243},
  howpublished = {CoNLL'2019},
}

@Misc{1609.07035,
  author = {Siddhartha Banerjee and Prasenjit Mitra and Kazunari Sugiyama},
  title  = {Abstractive Meeting Summarization Using Dependency Graph Fusion},
  year   = {2016},
  eprint = {arXiv:1609.07035},
}

@InProceedings{shang-etal-2018-unsupervised,
  author    = {Shang, Guokan and Ding, Wensi and Zhang, Zekun and Tixier, Antoine and Meladianos, Polykarpos and Vazirgiannis, Michalis and Lorr{\'e}, Jean-Pierre},
  title     = {Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year      = {2018},
  pages     = {664--674},
  address   = {Melbourne, Australia},
  month     = jul,
  publisher = {Association for Computational Linguistics},
  abstract  = {We introduce a novel graph-based framework for abstractive meeting speech summarization that is fully unsupervised and does not rely on any annotations. Our work combines the strengths of multiple recent approaches while addressing their weaknesses. Moreover, we leverage recent advances in word embeddings and graph degeneracy applied to NLP to take exterior semantic knowledge into account, and to design custom diversity and informativeness measures. Experiments on the AMI and ICSI corpus show that our system improves on the state-of-the-art. Code and data are publicly available, and our system can be interactively tested.},
  doi       = {10.18653/v1/P18-1062},
  url       = {https://www.aclweb.org/anthology/P18-1062},
}

@Misc{1802.05365,
  author = {Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
  title  = {Deep contextualized word representations},
  year   = {2018},
  eprint = {arXiv:1802.05365},
}

@Article{radford2019language,
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  title  = {Language Models are Unsupervised Multitask Learners},
  year   = {2019},
}

@Misc{1909.11942,
  author = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  title  = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  year   = {2019},
  eprint = {arXiv:1909.11942},
}

@Misc{1903.10318,
  author = {Yang Liu},
  title  = {Fine-tune BERT for Extractive Summarization},
  year   = {2019},
  eprint = {arXiv:1903.10318},
}

@Comment{jabref-meta: databaseType:bibtex;}
