% Encoding: UTF-8

@Misc{1706.03762,
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  title  = {Attention Is All You Need},
  year   = {2017},
  eprint = {arXiv:1706.03762},
}

@InProceedings{Mccowan05theami,
  author    = {I. Mccowan and G. Lathoud and M. Lincoln and A. Lisowska and W. Post and D. Reidsma and P. Wellner},
  title     = {The AMI Meeting Corpus},
  booktitle = {In: Proceedings Measuring Behavior 2005, 5th International Conference on Methods and Techniques in Behavioral Research. L.P.J.J. Noldus, F. Grieco, L.W.S. Loijens and P.H. Zimmerman (Eds.), Wageningen: Noldus Information Technology},
  year      = {2005},
}

@InProceedings{Janin,
  author    = {A. Janin and D. Baron and J. Edwards and D. Ellis and D. Gelbart and N. Morgan and B. Peskin and T. Pfau and E. Shriberg and A. Stolcke and C. Wooters},
  title     = {The {ICSI} Meeting Corpus},
  booktitle = {2003 {IEEE} International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. ({ICASSP} {\textquotesingle}03).},
  year      = {2003},
  publisher = {{IEEE}},
  doi       = {10.1109/icassp.2003.1198793},
}

@Article{devlin2018bert,
  author  = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title   = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  journal = {arXiv preprint arXiv:1810.04805},
  year    = {2018},
}

@InProceedings{rajpurkar-etal-2016-squad,
  author    = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  title     = {{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  year      = {2016},
  pages     = {2383--2392},
  address   = {Austin, Texas},
  month     = nov,
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/D16-1264},
  url       = {https://www.aclweb.org/anthology/D16-1264},
}

@Misc{1804.07461,
  author = {Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
  title  = {GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  year   = {2018},
  eprint = {arXiv:1804.07461},
}

@Misc{1908.08345,
  author = {Yang Liu and Mirella Lapata},
  title  = {Text Summarization with Pretrained Encoders},
  year   = {2019},
  eprint = {arXiv:1908.08345},
}

@Article{1902.09243,
  author       = {Haoyu Zhang and Jianjun Xu and Ji Wang},
  title        = {Pretraining-Based Natural Language Generation for Text Summarization},
  year         = {2019},
  eprint       = {arXiv:1902.09243},
  howpublished = {CoNLL'2019},
}

@Misc{1609.07035,
  author = {Siddhartha Banerjee and Prasenjit Mitra and Kazunari Sugiyama},
  title  = {Abstractive Meeting Summarization Using Dependency Graph Fusion},
  year   = {2016},
  eprint = {arXiv:1609.07035},
}

@InProceedings{shang-etal-2018-unsupervised,
  author    = {Shang, Guokan and Ding, Wensi and Zhang, Zekun and Tixier, Antoine and Meladianos, Polykarpos and Vazirgiannis, Michalis and Lorr{\'e}, Jean-Pierre},
  title     = {Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year      = {2018},
  pages     = {664--674},
  address   = {Melbourne, Australia},
  month     = jul,
  publisher = {Association for Computational Linguistics},
  abstract  = {We introduce a novel graph-based framework for abstractive meeting speech summarization that is fully unsupervised and does not rely on any annotations. Our work combines the strengths of multiple recent approaches while addressing their weaknesses. Moreover, we leverage recent advances in word embeddings and graph degeneracy applied to NLP to take exterior semantic knowledge into account, and to design custom diversity and informativeness measures. Experiments on the AMI and ICSI corpus show that our system improves on the state-of-the-art. Code and data are publicly available, and our system can be interactively tested.},
  doi       = {10.18653/v1/P18-1062},
  url       = {https://www.aclweb.org/anthology/P18-1062},
}

@Misc{1802.05365,
  author = {Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
  title  = {Deep contextualized word representations},
  year   = {2018},
  eprint = {arXiv:1802.05365},
}

@Article{radford2019language,
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  title  = {Language Models are Unsupervised Multitask Learners},
  year   = {2019},
}

@Misc{1909.11942,
  author = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  title  = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  year   = {2019},
  eprint = {arXiv:1909.11942},
}

@Misc{1903.10318,
  author = {Yang Liu},
  title  = {Fine-tune BERT for Extractive Summarization},
  year   = {2019},
  eprint = {arXiv:1903.10318},
}

@Article{Hochreiter1997,
  author    = {Sepp Hochreiter and J\"{u}rgen Schmidhuber},
  title     = {Long Short-Term Memory},
  journal   = {Neural Computation},
  year      = {1997},
  volume    = {9},
  number    = {8},
  pages     = {1735--1780},
  month     = nov,
  doi       = {10.1162/neco.1997.9.8.1735},
  publisher = {{MIT} Press - Journals},
  url       = {https://doi.org/10.1162/neco.1997.9.8.1735},
}

@Misc{Hochreiter01gradientflow,
  author = {Sepp Hochreiter and Yoshua Bengio and Paolo Frasconi and JÃ¼rgen Schmidhuber},
  title  = {Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies},
  year   = {2001},
}

@Misc{1409.0473,
  author = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  title  = {Neural Machine Translation by Jointly Learning to Align and Translate},
  year   = {2014},
  eprint = {arXiv:1409.0473},
}

@Misc{1512.03385,
  author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
  title  = {Deep Residual Learning for Image Recognition},
  year   = {2015},
  eprint = {arXiv:1512.03385},
}

@Misc{1607.06450,
  author = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
  title  = {Layer Normalization},
  year   = {2016},
  eprint = {arXiv:1607.06450},
}

@Misc{annotated.transformer,
  title        = {The Annotated Transformer},
  howpublished = {\url{http://nlp.seas.harvard.edu/2018/04/03/attention.html}},
  month        = apr,
  year         = {2018},
  note         = {Accessed: 2020-02-10},
}

@InProceedings{kitaev2020reformer,
  author    = {Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
  title     = {Reformer: The Efficient Transformer},
  booktitle = {International Conference on Learning Representations},
  year      = {2020},
  url       = {https://openreview.net/forum?id=rkgNKkHtvB},
}

@Misc{amiWebsite,
  title        = {AMI Corpus},
  howpublished = {\url{http://groups.inf.ed.ac.uk/ami/corpus/}},
  year         = {2006},
  note         = {Accessed: 2020-02-11},
}

@InProceedings{hu2019texar,
  author    = {Hu, Zhiting and Shi, Haoran and Tan, Bowen and Wang, Wentao and Yang, Zichao and Zhao, Tiancheng and He, Junxian and Qin, Lianhui and Wang, Di and others},
  title     = {Texar: A Modularized, Versatile, and Extensible Toolkit for Text Generation},
  booktitle = {ACL 2019, System Demonstrations},
  year      = {2019},
}

@Misc{1608.05859,
  author = {Ofir Press and Lior Wolf},
  title  = {Using the Output Embedding to Improve Language Models},
  year   = {2016},
  eprint = {arXiv:1608.05859},
}

@Article{Carletta2003,
  author    = {Jean Carletta and Stefan Evert and Ulrich Heid and Jonathan Kilgour and Judy Robertson and Holger Voormann},
  title     = {The {NITE} {XML} Toolkit: Flexible annotation for multimodal language data},
  journal   = {Behavior Research Methods, Instruments, {\&} Computers},
  year      = {2003},
  volume    = {35},
  number    = {3},
  pages     = {353--363},
  month     = aug,
  doi       = {10.3758/bf03195511},
  publisher = {Springer Science and Business Media {LLC}},
  url       = {https://doi.org/10.3758/bf03195511},
}

@Misc{tensorflow2015-whitepaper,
  author = {Mart\'{\i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
  title  = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
  year   = {2015},
  note   = {Software available from tensorflow.org},
  url    = {https://www.tensorflow.org/},
}

@InCollection{NEURIPS2019_9015,
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Advances in Neural Information Processing Systems 32},
  publisher = {Curran Associates, Inc.},
  year      = {2019},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {8024--8035},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
}

@Article{article,
  author  = {Kingma, Diederik and Ba, Jimmy},
  title   = {Adam: A Method for Stochastic Optimization},
  journal = {International Conference on Learning Representations},
  year    = {2014},
  month   = {12},
}

@InProceedings{lin-2004-rouge,
  author    = {Lin, Chin-Yew},
  title     = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
  booktitle = {Text Summarization Branches Out},
  year      = {2004},
  pages     = {74--81},
  address   = {Barcelona, Spain},
  month     = jul,
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/W04-1013},
}

@InBook{Ting2010,
  pages     = {781--781},
  title     = {Precision and Recall},
  publisher = {Springer US},
  year      = {2010},
  author    = {Ting, Kai Ming},
  editor    = {Sammut, Claude and Webb, Geoffrey I.},
  address   = {Boston, MA},
  isbn      = {978-0-387-30164-8},
  booktitle = {Encyclopedia of Machine Learning},
  doi       = {10.1007/978-0-387-30164-8_652},
  url       = {https://doi.org/10.1007/978-0-387-30164-8_652},
}

@InBook{Aggarwal2018,
  pages     = {1--52},
  title     = {An Introduction to Neural Networks},
  publisher = {Springer International Publishing},
  year      = {2018},
  author    = {Aggarwal, Charu C.},
  address   = {Cham},
  isbn      = {978-3-319-94463-0},
  abstract  = {Artificial neural networks are popular machine learning techniques that simulate the mechanism of learning in biological organisms. The human nervous system contains cells, which are referred to as neurons. The neurons are connected to one another with the use of axons and dendrites, and the connecting regions between axons and dendrites are referred to as synapses. These connections are illustrated in FigureÂ 1.1(a). The strengths of synaptic connections often change in response to external stimuli. This change is how learning takes place in living organisms.},
  booktitle = {Neural Networks and Deep Learning: A Textbook},
  doi       = {10.1007/978-3-319-94463-0_1},
  url       = {https://doi.org/10.1007/978-3-319-94463-0_1},
}

@InProceedings{oya-etal-2014-template,
  author    = {Oya, Tatsuro and Mehdad, Yashar and Carenini, Giuseppe and Ng, Raymond},
  title     = {A Template-based Abstractive Meeting Summarization: Leveraging Summary and Source Text Relationships},
  booktitle = {Proceedings of the 8th International Natural Language Generation Conference ({INLG})},
  year      = {2014},
  pages     = {45--53},
  address   = {Philadelphia, Pennsylvania, U.S.A.},
  month     = jun,
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/W14-4407},
  url       = {https://www.aclweb.org/anthology/W14-4407},
}

@InProceedings{10.1007/978-3-030-20521-8_53,
  author    = {Jacquenet, Francois and Bernard, Marc and Largeron, Christine},
  title     = {Meeting Summarization, A Challenge for Deep Learning},
  booktitle = {Advances in Computational Intelligence},
  year      = {2019},
  editor    = {Rojas, Ignacio and Joya, Gonzalo and Catala, Andreu},
  pages     = {644--655},
  address   = {Cham},
  publisher = {Springer International Publishing},
  abstract  = {Text summarization is one of the challenges of Natural Language Processing. Given the volume of texts produced daily on the Internet, managers can no longer have an exhaustive reading of current events, or progress reports from their employees, etc. They urgently need tools to automatically produce a summary of this flow of information. As a first approach, extractive summarization tools have been produced and there are now commercial tools available. However, this family of systems is not well suited to certain types of texts such as written transcriptions of dialogues or meetings. In that case, abstractive summarization tools are needed. Research in that field is very old but has been particularly stimulated since the mid-2010s by the recent successes of deep learning. This paper presents a short survey of deep learning approaches to abstractive text summarization and then highlights the various challenges that will have to be solved in the coming years to deal with meeting summaries in order to be able to provide a text summarization tool that generates good quality summaries.},
  isbn      = {978-3-030-20521-8},
}

@Comment{jabref-meta: databaseType:bibtex;}
