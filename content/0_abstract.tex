\thispagestyle{empty}
\section*{Kurzdarstellung}
\label{sec:kurzdarstellung}

Die vorliegende Bachelorarbeit untersucht, wie ein modernes vortrainiertes neuronales Netz, wie z.B. Googles BERT \cite{devlin2018bert}, zur abstrakten Textzusammenfassung von Meetings genutzt werden kann.

Im Rahmen dieser Arbeit wird ein Tranformer Netz \cite{1706.03762} implementiert, das BERT als seinen Encoder nutzt.
Der Decoder des Netzes ist ein normaler, nicht vortrainierter, Transormer-Decoder.
Das Netz wird hierfür mit Daten aus dem AMI Meeting Corpus \cite{Mccowan05theami} und dem ICSI Meeting Corpus \cite{Janin} trainiert.
Es wird dafür ein 2-stufiges Verfahren genutzt.
Im ersten Schritt wird das Netzwerk darauf trainiert, eine Menge von Dialogen auf einen einzelnen Satz abzubilden, der aus der abstrakten Zusammenfassung des Meetings stammt.
Im zweiten Schritt werden die Meetings nach Themen aufgeteilt und anschließend alle Dialoge eines Themas als Eingabe für das neuronale Netz genutzt.
Die zusammengehängte Ausgabe des Netzes für alle Themen eines Meetings ist dann die fertige Zusammenfassung des gesamten Meetings.
Dies dient zum einen dazu, den benötigten Speicherbedarf von BERT bei langen Textsequenzen zu reduzieren.
Zum anderen kann damit auch das Problem umgangen werden, dass BERT nur mit Textsequenzen bis zu einer Länge von 512 vortrainiert wurde \cite[p.~13]{devlin2018bert}.

Die Arbeit zeigt, dass ein datengetriebener Ansatz in der Theorie funktioniert.
Allerdings haben die Ergebnisse eine sehr starke Neigung dazu, Informationen aus den Traingsdaten in die Zusammenfassung einfliesen zu lassen, selbst wenn diese nicht in der Eingabe vorhanden sind.
Dies ist besonders offensichtlich, wenn ein Netz, das nur mit dem AMI Korpus trainiert wurde, auf den ICSI Corpus angewendet wird.
Die hierbei erziehlten Ergebnisse sind meist sehr schlecht.
Hieraus lässt sich ableiten, dass wesentlich mehr Trainigsdaten notwendig sind, um praxistaugliche Ergebnisse zu erzielen, die unabhängig von einem stark eingegrenzten Kontext, wie es bei den AMI Szenario-Meetings der Fall ist, sind.


\section*{Abstract}
\label{sec:abstract}

This work analyzes how a state-of-the-art pretrained neural network - such as Google's BERT \cite{devlin2018bert} - can be fine-tuned to the task of abstractive text summarization in the context of meetings.

As part of this work a Transformer network \cite{1706.03762} is developed, that uses BERT as its encoder and a plain decoder.
It is trained on the AMI Meeting Corpus \cite{Mccowan05theami} as well as the ICSI Meeting Corpus \cite{Janin}.
To circumvent the high memory usage of BERT at long sequence lengths and the fact that BERT is pre-trained with a maxiumum sequence length of 512 \cite[p.~13]{devlin2018bert}, the summarization is performed using a two-step approach.
First, the network is trained to summarize \(n\) dialouge acts to a single sentence of the meeting's abstractive summary.
Afterwards, a whole meeting is split by its topics and all the dialouge acts of a topic are fed into the trained network as its input.
The concatenated outputs of the Transformer for every topic is the final summary.

The work proofs, that a data-driven approach is feasible in theory.
However, the results have a strong bias towards the context of the meeting and cross-corpus validation on AMI and ICSI shows a very poor performance.
This indicates, that far more training data is necessary for practicable results outside of a very topic-specific context like the scenario meetings of the AMI corpus. 