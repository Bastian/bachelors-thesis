\thispagestyle{empty}
\section*{Kurzdarstellung}
\label{sec:kurzdarstellung}

Die vorliegende Bachelorarbeit untersucht, wie ein modernes vortrainiertes neuronales Netz, wie \zB Googles BERT \cite{devlin2018bert}, zur abstrakten Textzusammenfassung von Meetings genutzt werden kann.

Im Rahmen dieser Arbeit wird ein Transformer Netz \cite{1706.03762} implementiert, das BERT als seinen Encoder nutzt.
Der Decoder des Netzes ist ein konventioneller, nicht vortrainierter Transformer-Decoder.
Das Netz wird mit Daten aus dem AMI Meeting Corpus \cite{Mccowan05theami} und aus dem ICSI Meeting Corpus \cite{Janin} trainiert.
Hierfür wird ein zweistufiges Verfahren angewendet.
Im ersten Schritt wird das Netzwerk darauf trainiert, eine Menge von Dialogen auf einen einzelnen Satz abzubilden, der aus der abstrakten Zusammenfassung des Meetings stammt.
Im zweiten Schritt werden die Meetings nach Themen aufgeteilt und anschließend werden alle Dialoge eines Themas als Eingabe für das neuronale Netz genutzt.
Die zusammengefügte Ausgabe des Netzes für alle Themen eines Meetings ist dann die fertige Zusammenfassung des gesamten Meetings.
Dieses zweistufige Verfahren dient zum einen dazu, den benötigten Speicherbedarf von BERT bei langen Textsequenzen zu reduzieren.
Zum anderen kann damit auch das Problem umgangen werden, dass BERT nur mit Textsequenzen bis zu einer Länge von 512 vortrainiert wurde \cite[p.~13]{devlin2018bert}.

Die Arbeit zeigt, dass ein datengetriebener Ansatz in der Theorie funktioniert.
Allerdings weisen die Ergebnisse eine starke Tendenz dazu auf, Informationen aus den Trainingsdaten in die Zusammenfassung einfließen zu lassen, selbst wenn diese nicht in der Eingabe vorhanden sind.
Dies ist besonders offensichtlich, wenn ein Netz, das nur mit dem AMI Korpus trainiert wurde, auf das ICSI Korpus angewendet wird.
Die hierbei erzielten Ergebnisse sind meist unzureichend.
Hieraus lässt sich ableiten, dass wesentlich mehr Trainingsdaten notwendig sind, um praxistaugliche Ergebnisse zu erzielen, die von einem stark eingegrenzten Kontext, wie es bei den AMI-Szenario-Meetings der Fall ist, unabhängig sind.

\section*{Abstract}
\label{sec:abstract}

This work analyzes how a state-of-the-art pretrained neural network---such as Google's Bidirectional Encoder Representations from Transformers (BERT) \cite{devlin2018bert}---can be fine-tuned to the task of abstractive text summarization in the context of business meetings. %TODO Maybe briefly explain what an abstractive summary is (compared to an extractive summary)

As part of this work, a Transformer network \cite{1706.03762} is developed that uses BERT as its encoder and a plain decoder.
It is trained on the AMI Meeting Corpus \cite{Mccowan05theami} as well as the ICSI Meeting Corpus \cite{Janin}.
To circumvent the high memory usage of BERT at long sequence lengths and the fact that BERT is pretrained with a maximum sequence length of 512 \cite[p.~13]{devlin2018bert}, the summarization is performed using a two-step approach.
First, the network is trained to summarize \(n\) dialogue acts into a single sentence of the meeting's abstractive summary.
Afterward, a whole meeting is split up by its topics, and all the dialogue acts of a topic are fed into the trained network as its input.
The concatenated outputs of the Transformer for every topic comprise the final summary.

This research shows that a data-driven approach is feasible in theory.
However, the results have a strong bias toward the context of the meeting, and cross-corpus validation on AMI and ICSI shows a very poor performance.
This indicates that far more training data is necessary for practicable results outside of a very topic-specific context like the scenario meetings of the AMI corpus. 