\chapter{Experiments}\label{ch:experiments}

Using the previously described neural network, two sets of experiments were performed.
This chapter presents the results of these experiments and the scores it achieved on the ROUGE metric.
For the second experiments, it compares the results to other summarization approaches.
It also introduces a simple baseline for better comparison.

% ==============
% INITIAL EXPERIMENTS
% ==============

\section{Initial Experiments}\label{sec:initial-experiments}

For the first experiments, the network is trained to summarize $n$ dialogue acts to one sentence of a meeting's abstract.
Results for the following combinations are reported:

\begin{itemize}
\item Trained on the AMI train dataset, validated on the AMI eval dataset, tested on the AMI test dataset (\textbf{AAA})
\item Trained on the AMI train dataset, validated on the AMI eval dataset, tested on the ICSI corpus (\textbf{AAI})
\item Trained, validated, and tested on the ICSI corpus, using a the split described in \cref{sec:icsi-corpus} (\textbf{III})
\item Trained on the ICSI train dataset, validated on the ICSI eval dataset, tested on the AMI test dataset (\textbf{IIA})
\item Trained, validated, and tested on a combined AMI and ICSI corpus, with all the datasets being merged with the datasets of the other corpus\footnote{\Eg\ the combined train set is the AMI train set + the ICSI train set} (\textbf{CCC})
\item Trained and validated on a combined AMI and ICSI corpus, tested on the AMI test data(\textbf{CCA})
\item Trained and validated on a combined AMI and ICSI corpus, tested on the AMI test data(\textbf{CCI})
\end{itemize}

A single training step took, on average, about $0.41$ seconds on a single GeForce RTX 2080 Ti GPU which in turn results in a training time of about one hour for the fastest variant (AAA) and three hours for the slowest variant (CCC).
The achieved ROUGE scores are shown in \cref{tab:initial-experiment-rouge}.
While the results for AAA, CCC, and CCA are good, the network failed to generalize enough to obtain useful results for the two cross-corpus variations AAI and IAA.
The network also achieves way worse results on the more diversified ICSI Meeting Corpus compared to the AMI Meeting Corpus with its narrow scenario.
Training the network on both corpora does not improve the test results for a single corpus.

\begin{table}[h]
\centering
\begin{tabular}{c|l|clll}
\textbf{Variant}              & \textbf{Training Steps}                                                                   & \textbf{Metric} & \textbf{Pre} & \textbf{Rec} & \textbf{F} \\ \hline
\multirow{3}{*}{\textbf{AAA}} & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}8625 steps\\ (approx. 141 epochs)\end{tabular}} & \textbf{R-1}    & 38.49       & 29.52       & 31.92     \\
                              &                                                                                            & \textbf{R-2}    & 17.18       & 13.51       & 14.28     \\
                              &                                                                                            & \textbf{R-L}    & 34.89       & 27.01       & 26.87     \\ \hline
\multirow{3}{*}{\textbf{AAI}} & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}8625 steps\\ (approx. 141 epochs)\end{tabular}}  & \textbf{R-1}    & 17.38         & 10.29         & 12.53       \\
                              &                                                                                            & \textbf{R-2}    & 1.14         & 0.59         & 0.76       \\
                              &                                                                                            & \textbf{R-L}    & 14.55         & 8.62         & 9.21       \\ \hline
\multirow{3}{*}{\textbf{III}} & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}13350 steps\\ (approx. 351 epochs)\end{tabular}}  & \textbf{R-1}    & 18.48         & 14.64         & 15.55       \\
                              &                                                                                            & \textbf{R-2}    & 3.55         & 2.93         & 2.99       \\
                              &                                                                                            & \textbf{R-L}    & 15.75         & 12.44         & 12.23       \\ \hline
\multirow{3}{*}{\textbf{IIA}} & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}13350 steps\\ (approx. 351 epochs)\end{tabular}}  & \textbf{R-1}    & 11.66         & 12.89         & 11.39       \\
                              &                                                                                            & \textbf{R-2}    & 0.61         & 0.73         & 0.63       \\
                              &                                                                                            & \textbf{R-L}    & 10.10         & 11.20         & 9.04       \\ \hline
\multirow{3}{*}{\textbf{CCC}} & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}27000 steps\\ (approx. 270 epochs)\end{tabular}}  & \textbf{R-1}    & 32.70         & 25.28         & 27.15       \\
                              &                                                                                            & \textbf{R-2}    & 13.09         & 10.33         & 10.93       \\
                              &                                                                                            & \textbf{R-L}    & 29.28         & 22.73         & 22.45       \\ \hline
\multirow{3}{*}{\textbf{CCA}} & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}27000 steps\\ (approx. 270 epochs)\end{tabular}}  & \textbf{R-1}    & 37.90         & 29.58         & 31.62       \\
                              &                                                                                            & \textbf{R-2}    & 17.00         & 13.40         & 14.18       \\
                              &                                                                                            & \textbf{R-L}    & 34.37         & 26.95         & 26.54       \\ \hline
\multirow{3}{*}{\textbf{CCI}} & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}27000 steps\\ (approx. 270 epochs)\end{tabular}}  & \textbf{R-1}    & 18.68         & 13.70         & 15.13       \\
                              &                                                                                            & \textbf{R-2}    & 2.57         & 2.07         & 2.19       \\
                              &                                                                                            & \textbf{R-L}    & 15.58         & 11.38         & 11.44       \\ \hline
\end{tabular}
\caption{ROUGE scores of the initial experiments}
\label{tab:initial-experiment-rouge}
\end{table}

\Cref{fig:initial-experiment-example} shows two hand-picked examples of generated summaries together with the input and the human summary.
It is easily noticeable that the network tends to overfit the data as the generated summary contains information that is not part of the actual input but can only be guessed if the setting of AMI's scenario meetings is taken into account (\eg, that the final report is always written by the project manager).
This explains the bad results for the AAI and IAA variations as the network tries to apply the setting of the AMI corpus to the ICSI corpus and vice versa.
It is also a strong indicator that more training data is necessary for the network to learn to generalize, especially considering the very specific context of the AMI scenario meetings.

\begin{figure}[h]
\begin{lstlisting}[numbers=none]
DA: and lea l delivered standard with a fruity colour, but not too
    not too much. but, the mai i think th the standard must be
    some kind of attractive flashy colours. so i think it will be 
    a better idea to have some flashy fruity colours as as a
    standard, and for the people who really want a more
    sophisticated, more traditional look, they're willing to pay
    that. 
HS: the remote will come in fruity colours as standard with the
    option of buying different exchangeable covers.
GS: the remote will be made of fruity colors.
------------------------------------------------------------------
DA: that's that's what i'm gonna write b between now. i'm going
    to finish my end report.
HS: the project manager will finish the final report.
MS: the project manager will create a final report
\end{lstlisting}
\caption[Comparison between human summary and machine-generated summary of dialogue acts for variant AAA]{Comparison between human summary (HS) and machine-generated summary (GS) of dialogue acts (DA) for variant AAA}
\label{fig:initial-experiment-example}
\end{figure}

However, the network is able to correctly interpret the context of a sentence and is---at least to a certain degree---able to generate new sentences.
The generated summary from the first example in \cref{fig:initial-experiment-example} is not present in the training data at all but constructed from multiple different sentences.
The generated summary from the second example is identical to a human summary that was used for training but for another input.
This human summary with the matching dialogue act in the training dataset is shown in \cref{fig:initial-experiment-training-example}.
However, the second example is the much more common case, and the network does not generate new sentences very often but only copies them from the training examples.
By reducing the number of training steps, the network tends to generate own sentences more often but at the cost of lower scores.

\begin{figure}[h]
\begin{lstlisting}[numbers=none]
DA: because I have to write the final report now.
HS: The project manager will create a final report
\end{lstlisting}
\caption{The training example, the summary of which the network copied in the second example of \cref{fig:initial-experiment-example}}
\label{fig:initial-experiment-training-example}
\end{figure}

% ================
% EXTENDED EXPERIMENTS
% ================

\section{Extended Experiments}

For the extended experiments, the topic segmentation described in \cref{ssec:ami-annotations} is used to split meeting transcripts into smaller pieces that are used as inputs for the same network trained in \cref{sec:initial-experiments}.
This results in one summary sentence for each topic of a meeting.
By concatenating all these summary sentences of one meeting, one can get a summary of the entire meeting.
For this experiment, only the AMI Meeting Corpus is used with the standard data split described in \cref{ssec:ami-segmentation-of-the-corpus} as there is no manual topic segmentation available for the ICSI Meeting Corpus. % TODO Maybe use the automatic segmentation with the automatic transcript?

An example of a generated summary is shown in \cref{fig:extended-experiment-example}.
The achieved ROUGE scores are shown in \cref{tab:extended-experiment-rouge}.

\begin{figure}[h]
\begin{lstlisting}[numbers=none]
the project manager gave an introduction to the goal of the
project, to create a trendy yet user-friendly remote. she
presented a long-range agenda for the whole project. the group
introduced themselves to each other and practiced with the meeting
room tools by drawing on the board. the project manager presented
the project budget, the projected price point, and the projected
profit aim for the project. then the group began a discussion
about their own experiences with remote controls to generate
initial design ideas for making the product user-friendly. they
discussed grouping features into a menu and adding an lcd display.
they also discussed the look of various materials that may be used 
n the design, in keeping with the company's goal to create
fashionable electronics. 
------------------------------------------------------------------
the group discussed making the remote universally compatible and
ergonomic design. the project manager states that the remote needs
to be original, trendy, and user - friendly. the team members then
participated in an exercise in which they drew their favorite
animals. the project manager discussed the financial goals of the
project, including the projected profit aim and price point for
the device. the team then discussed their experiences using
remotes in the past and what features to consider implementing in
the remote. the project manager closes the meeting. the marketing
expert discussed his findings from trend watching reports. the
project manager closed the meeting. 
\end{lstlisting}
\caption[Comparison of human summary with machine-generated summary]{Comparison of human summary (top) with machine-generated summary (bottom)}
\label{fig:extended-experiment-example}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{@{}clll@{}}
\toprule
\textbf{Metric} & \multicolumn{1}{c}{\textbf{Pre}} & \multicolumn{1}{c}{\textbf{Rec}} & \multicolumn{1}{c}{\textbf{F}} \\ \midrule
\textbf{R-1}    & 36.65                           & 32.60                           & 33.54                         \\
\textbf{R-2}    & 13.68                           & 13.02                           & 12.90                         \\
\textbf{R-L}    & 33.88                           & 30.14                           & 29.44                         \\ \bottomrule
\end{tabular}
\caption{ROUGE scores of the extended experiments}
\label{tab:extended-experiment-rouge}
\end{table}

\paragraph{Comparison to Other Approaches}

The metrics used to report results are very different, and many of them are only reported for the AMI Meeting Corpus.
Some researchers only report the ROUGE-1 and ROUGE-2 scores using the F1 measure, while others also report ROUGE-L or ROUGE-SU4 scores with recall, precision, and F1 measures.
For this reason, this thesis only compares ROUGE-1 and ROUGE-2 scores using the F1 measure for the AMI Meeting Corpus as these scores are reported for nearly every approach.

The results of this work's neural approach are compared to the following approaches for abstractive meeting summarization:
\begin{itemize}
\item The results from \cite{shang-etal-2018-unsupervised}, using their best-performing approach on AMI (\textbf{Ref-A})
\item The results from \cite{oya-etal-2014-template}, using their 15-segment (\textbf{Ref-B}) and 20-segment system (\textbf{Ref-C})
\item The results from \cite{mehdad-etal-2013-abstractive}, using their full model (\textbf{Ref-D})
\end{itemize}

These scores are listed in \cref{tab:comparison} together with the neural network's scores and a baseline that is described later in this section.
When just comparing the plain ROUGE scores, the neural approach achieves competitive results for ROUGE-1 scores and surpasses the ROUGE-2 scores of any of the other approaches.
However, this comparison is not fair as the other approaches only use the dialogue acts or transcripts to generate their summaries.
While this is also true for the model of this work, it has access to the abstractive summaries of the training set during its training.
Because all of AMI's scenario meetings have a very similar structure, the abstracts in the training data tend to be very similar to the abstracts in the test dataset.
This gives the network an unfair advantage over the other approaches and makes the results hardly comparable.

\begin{table}[]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
                       & \textbf{R-1}     & \textbf{R-2}     \\ \midrule
\textbf{Neural (Ours)} & 33.54                & \underline{\textbf{12.90}} \\
\textbf{Ref-A}         & \underline{\textbf{37.86}} & 7.84                 \\
\textbf{Ref-B}         & 30.6                 & 6.8                  \\
\textbf{Ref-C}         & 31.5                 & 6.7                  \\
\textbf{Ref-D}         & 28.7                 & 4.2                  \\
\textbf{Baseline}      & 28.32                & 8.48                 \\ \bottomrule
\end{tabular}
\caption{Comparison of different approaches for abstractive meeting summarization and a baseline}
\label{tab:comparison}
\end{table}

To still have a baseline that results can be compared to, one can randomly pick a summary sentence from the training data for every topic.
If the network is able to understand the meaning of its input, it should outperform this baseline.
This baseline is also included in the comparison \cref{tab:comparison}, and all scores are also shown in \cref{tab:extended-experiment-rouge-random}.
When comparing the network's scores with this baseline, the results of the network indeed outperform the baseline by a significant margin.
The fact that even this simple baseline achieves higher ROUGE-2 scores than any of the unsupervised approaches is a clear indicator that the access to the abstracts of the training data is indeed an unfair advantage.

\begin{table}[h]
\centering
\begin{tabular}{@{}clll@{}}
\toprule
\textbf{Metric} & \multicolumn{1}{c}{\textbf{Pre}} & \multicolumn{1}{c}{\textbf{Rec}} & \multicolumn{1}{c}{\textbf{F}} \\ \midrule
\textbf{R-1}    & 26.25                           & 32.64                           & 28.32                         \\
\textbf{R-2}    & 07.88                           & 09.92                           & 08.48                         \\
\textbf{R-L}    & 23.84                           & 29.57                           & 24.40                         \\ \bottomrule
\end{tabular}
\caption{ROUGE scores of random baseline}
\label{tab:extended-experiment-rouge-random}
\end{table}

\paragraph{Issues with the Generated Summary}

The generated summaries come with some issues that are not visible when just looking at the ROUGE scores.
One example is duplicate sentences, or sentences that carry the same meaning, like the two sentences "The project manager closes the meeting" and "The project manager closed the meeting," which are both part of the same summary shown in \cref{fig:extended-experiment-example}.
While this issue could be solved by some clever filtering of duplicates, a more severe problem is wrong information in the summaries (\eg, the sentence "The marketing expert discussed his findings from trend watching reports" is incorrect for the meeting from \cref{fig:extended-experiment-example}).